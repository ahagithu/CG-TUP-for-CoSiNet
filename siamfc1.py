from __future__ import absolute_import, division, print_function
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import time
import cv2
import sys
import torchvision.models.vgg

# å°†æ¨¡å—è·¯å¾„åŠ åˆ°å½“å‰æ¨¡å—æ‰«æçš„è·¯å¾„é‡Œ
from torch.utils import model_zoo

# sys.path.append("/home/asus/ly/SiamDUL/siamfc")
import os
from collections import namedtuple
from torch.optim.lr_scheduler import ExponentialLR
from torch.utils.data import DataLoader
from got10k.trackers import Tracker

import sys

sys.path.append(os.path.abspath('.'))

# import sys
# dir_mytest = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# sys.path.insert(0, dir_mytest)

from siamfc import ops
# from siamfc import backbones
# from . import backbones
from siamfc.heads import SiamFC  # from .heads import DASiamRPN
from siamfc.losses import GHMCLoss
from siamfc.losses import FocalLoss
from siamfc.losses import BalancedLoss  # from .losses import BalancedLoss
from siamfc.datasets import Pair  # from .datasets import Pair
from siamfc.transforms import SiamFCTransforms  # from .transforms import SiamFCTransforms
from siamfc import backbones

from siamfc.attention import GlobalAttentionBlock, CBAM
from siamfc.backbones import SELayer1, ECALayer, ECALayer1
from siamfc.dcn import DeformConv2d
from siamfc.psp import PSA

__all__ = ['TrackerSiamFC']  # ç›´æ¥è¿›å…¥TrackerSiamFC


# æ¨¡å‹æ€»ä½“ç»“æ„
class Net(nn.Module):  # ç»§æ‰¿nn.Module

    def __init__(self, backbone, backbone1, head):
        super(Net, self).__init__()
        self.head = head
        self.backbone1 = backbone1
        self.backbone = backbone
        self.conv = nn.Sequential(nn.Conv2d(2, 1, 1))
        # self.att = att

        # self.tematt = GlobalAttentionBlock()
        # self.tematt = PSA(channel=256,reduction=8)
        self.tematt = ECALayer(256)

        self.detatt = CBAM(256)
        # self.attse = nn.Sequential(SELayer1(256))
        self.attse = ECALayer1(256)

        # self.eca = ECALayer(256)

        # self.convat = nn.Sequential(nn.Conv2d(512, 256, 1))

    # self.dcn = DeformConv2d()
    # self.satt = CBAM(1)

    def forward(self, z, x):
        z1 = self.backbone(z)  # vgg
        x1 = self.backbone(x)
        z2 = self.backbone1(z)  # alexnet
        x2 = self.backbone1(x)

        # z1 = self.dcn(z1)
        # z2 = self.dcn(z2)
        # x1 = self.dcn(x1)
        # x2 = self.dcn(x2)

        zf1 = self.tematt(z1)
        zf11 = self.attse(z1)
        zf11 = zf11 + z1
        z1 = zf1 + zf11

        zf2 = self.tematt(z2)
        zf22 = self.attse(z2)
        zf22 = zf22 + z2
        z2 = zf2 + zf22

        xf1 = self.detatt(x1)
        xf11 = self.attse(x1)
        xf11 = xf11 + x1
        x1 = xf1 + xf11

        xf2 = self.detatt(x2)
        xf22 = self.attse(x2)
        xf22 = xf22 + x2
        x2 = xf2 + xf22

        out1 = self.head(z1, x1)
        out2 = self.head(z2, x2)

        # out1 = self.dcn(out1)
        # out2 = self.dcn(out2)
        out11 = out1 + out2
        out22 = out1 * out2

        out = torch.cat([out11, out22], dim=1)
        out = self.conv(out)

        # outa = self.satt(out)
        # out = out+outa
        # out = out1 + out2
        return out


class TrackerSiamFC(Tracker):  # ç»§æ‰¿got10k ä¹Ÿå°±æ˜¯TrackerSiamFCå¯ä»¥è°ƒç”¨Trackeré‡Œçš„ç±»

    def __init__(self, net_path=None, **kwargs):  # å®šä¹‰selfçš„å±æ€§
        super(TrackerSiamFC, self).__init__('SiamFC', True)  # ç»§æ‰¿çˆ¶ç±»Trackerçš„å±æ€§nameå’Œis_deterministic
        # self.name = DASiamRPN
        # self.is_deterministic = True
        self.cfg = self.parse_args(**kwargs)

        # setup GPU device if available
        self.cuda = torch.cuda.is_available()
        self.device = torch.device('cuda:0' if self.cuda else 'cpu')

        # setup model  å…ˆæ–°å»ºä¸€ä¸ªmodelï¼Œç„¶ååŠ è½½é¢„è®­ç»ƒå¥½çš„æƒé‡ï¼Œå°±ä¸ç”¨å†è®­ç»ƒä¸€æ¬¡äº†ã€‚æ‰€ä»¥åˆ›å»ºçš„modelå’Œä¼ å…¥çš„pretrained weightsä¸€å®šæ˜¯è¦å¯¹åº”çš„
        self.net = Net(
            backbone=backbones.vgg(),
            # backbone=backbones.resnet34(),
            backbone1=backbones.AlexNetV(),
            head=SiamFC(self.cfg.out_scale),
            # head2=SiamFC(self.cfg.out_scale, False),
        )
        ops.init_weights(self.net),

        # load checkpoint if provided æ£€æŸ¥æ˜¯å¦ä¼ å…¥äº†pretrained weightsï¼Œæœ‰çš„è¯å°±è¯»è¿›å»ï¼Œæ²¡æœ‰çš„è¯ï¼Œå°±æ²¡æœ‰äº†
        if net_path is not None:  # net_path -> siamfc_alexmet_e50.pth
            self.net.load_state_dict(torch.load(
                net_path,
                map_location=lambda storage, load_state_dictc: storage),
                strict=False)  # # Load all tensors onto the CPU, using a function
        self.net = self.net.to(self.device)  # siamfc_alexmet_e50.pth å…ˆç”¨cpuè¯»ç„¶åå†è½¬åˆ°gpu

        # Variables pour le template update
        self.frame_count = 0
        self.best_response = 0
        self.best_template = None
        self.current_template_z1 = None
        self.current_template_z2 = None
        self.response_history = []
        self.update_log = []  # Pour stocker l'historique des mises Ã  jour

        # setup criterion
        # self.criterion = FocalLoss()
        self.criterion = BalancedLoss()

        # setup optimizerBCE
        self.optimizer = optim.SGD(  # æ¢¯åº¦ä¸‹é™æ³• å¸¦momentumçš„
            self.net.parameters(),
            lr=self.cfg.initial_lr,  # å­¦ä¹ ç‡
            weight_decay=self.cfg.weight_decay,  # è®¾ç½®æƒå€¼è¡°å‡ï¼Œç›®çš„æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆ
            momentum=self.cfg.momentum)

        # setup lr scheduler è¿™å—ä¸æ‡‚ï¼Œè¿˜æ²¡çœ‹
        # gamma = np.power(  # np.power(a,b) æ•°ç»„çš„å¹‚è®¡ç®— açš„bæ¬¡æ–¹
        #     self.cfg.ultimate_lr / self.cfg.initial_lr,  # gamma = le-3^0.02
        #     1.0 / self.cfg.epoch_num)
        gamma = np.power(  # np.power(a,b) æ•°ç»„çš„å¹‚è®¡ç®— açš„bæ¬¡æ–¹
            self.cfg.ultimate_lr / self.cfg.initial_lr,  # gamma = le-3^0.02
            1.0 / self.cfg.epoch_num)
        self.lr_scheduler = ExponentialLR(self.optimizer, gamma)  # 0.87
        # m = 1;
        # if m >= 65:
        #     self.lr_scheduler = ExponentialLR(self.optimizer, 0.87)
        #     m = m + 1
        # elif m >= 75:
        #     self.lr_scheduler = ExponentialLR(self.optimizer, 0.7)
        #     m = m + 1
        # elif m >= 85:
        #     self.lr_scheduler = ExponentialLR(self.optimizer, 0.6)
        #     m = m + 1
        # else:
        #     self.lr_scheduler = ExponentialLR(self.optimizer, 0.5)

    def parse_args(self, **kwargs):
        # default parameters
        cfg = {
            # basic parameters
            'out_scale': 0.001,
            'exemplar_sz': 127,  # æ¨¡æ¿å›ºå®šä¸º127*127
            'instance_sz': 255,  # current frameå›ºå®šä¸º255*255
            'context': 0.5,
            # inference parameters
            'scale_num': 3,
            'scale_step': 1.0375,
            # 'scale_step': 1.0482,
            # 'scale_lr': 0.83,
            'scale_lr': 0.59,
            'scale_penalty': 0.9745,
            'window_influence': 0.176,
            'response_sz': 17,  # ç›¸åº”å°ºå¯¸æ˜¯å›ºå®šçš„17Ã—17
            'response_up': 16,  # 16Ã—17=272 æŠŠcroped patchè¿˜åŸæˆorignal image
            'total_stride': 8,
            # train parameters
            'epoch_num': 70,
            'batch_size': 16,
            'num_workers': 8,
            'initial_lr': 1e-3, # 1e-2 is the default value
            # 'ultimate_lr': 1e-5,
            'ultimate_lr': 1e-5,
            'weight_decay': 5e-4,
            'momentum': 0.9,
            'r_pos': 16,  # radius for positive pairs è®ºæ–‡2.2å°èŠ‚
            'r_neg': 0,  # radius for negative pairs

            # Template update parameters
            # Template update parameters - Configuration avancÃ©e
            'template_update_interval': 5,  # Mise Ã  jour plus frÃ©quente
            'template_learning_rate': 0.015,  # Taux Ã©quilibrÃ©
            'quality_threshold': 0.012,  # Seuil plus sÃ©lectif
            'response_normalization_factor': 8.0,  # Ajustement fin
            'show_update_conditions': True,
            # Nouvelles paramÃ¨tres avancÃ©s
            'adaptive_update': True,  # Activer l'update adaptatif
            'max_updates_per_sequence': 20,  # Limiter le nombre total d'updates
            'min_quality_gain': 0.05,  # Gain de qualitÃ© minimum requis
            'emergency_update_threshold': 0.005,  # Seuil d'urgence pour rÃ©cupÃ©ration
        }

        for key, val in kwargs.items():  # å¦‚æœä¼ äº†å­—æ®µåç§°è¿›æ¥ï¼Œå°±å¯¹å­—æ®µåç§°çš„æ•°å€¼è¿›è¡Œæ›´æ–°
            if key in cfg:
                cfg.update({key: val})
        return namedtuple('Config', cfg.keys())(
            **cfg)  # è¿”å›å…·åå…ƒç»„ï¼Œå¯ä»¥ç”¨indexè®¿é—®ä¹Ÿå¯ä»¥ç”¨å­—æ®µåç§°è®¿é—®ã€‚å¦‚è®¿é—®â€˜out_scaleâ€™å¯ä»¥æ˜¯Config(0)ä¹Ÿå¯ä»¥æ˜¯Config['out_scale']

    def _update_template(self, img, max_response, scale_id):
        """Mise Ã  jour du template basÃ©e sur la qualitÃ© du tracking"""
        self.frame_count += 1

        # Historique des rÃ©ponses
        self.response_history.append(max_response)
        if len(self.response_history) > 20:  # Garder les 20 derniÃ¨res rÃ©ponses
            self.response_history.pop(0)

        # Calcul des mÃ©triques pour la dÃ©cision
        normalized_response = max_response / self.cfg.response_normalization_factor
        avg_response = np.mean(self.response_history) if self.response_history else max_response
        adaptive_threshold = avg_response * 0.6  # 60% de la rÃ©ponse moyenne

        # Conditions pour la mise Ã  jour
        condition_interval = (self.frame_count % self.cfg.template_update_interval == 0)
        condition_confidence = (max_response > self.cfg.quality_threshold)
        condition_adaptive = (max_response > adaptive_threshold)

        update_condition = (
                condition_confidence and
                condition_adaptive and
                condition_interval
        )

        # Affichage des conditions si demandÃ©
        if self.cfg.show_update_conditions:
            print(f"Frame {self.frame_count}: Response={max_response:.3f}, "
                  f"Normalized={normalized_response:.3f}, "
                  f"Avg={avg_response:.3f}, "
                  f"AdaptThresh={adaptive_threshold:.3f}, "
                  f"Interval={condition_interval}, "
                  f"Confidence={condition_confidence}, "
                  f"Adaptive={condition_adaptive}")

        if update_condition:
            try:
                # Extraire le nouveau template
                z_new = ops.crop_and_resize(
                    img, self.center, self.z_sz,
                    out_size=self.cfg.exemplar_sz,
                    border_value=self.avg_color)

                z_new = torch.from_numpy(z_new).to(
                    self.device).permute(2, 0, 1).unsqueeze(0).float()

                # Calculer les nouvelles caractÃ©ristiques
                z1_new = self.net.backbone(z_new)
                z2_new = self.net.backbone1(z_new)

                zf1_new = self.net.tematt(z1_new)
                zf11_new = self.net.attse(z1_new)
                zf11_new = zf11_new + z1_new
                kernel1_new = zf1_new + zf11_new

                zf2_new = self.net.tematt(z2_new)
                zf22_new = self.net.attse(z2_new)
                zf22_new = zf22_new + z2_new
                kernel2_new = zf2_new + zf22_new

                # Mise Ã  jour linÃ©aire
                lr = self.cfg.template_learning_rate
                self.kernel1 = (1 - lr) * self.kernel1 + lr * kernel1_new
                self.kernel2 = (1 - lr) * self.kernel2 + lr * kernel2_new

                # Log de la mise Ã  jour
                update_info = {
                    'frame': self.frame_count,
                    'response': max_response,
                    'normalized_response': normalized_response,
                    'adaptive_threshold': adaptive_threshold,
                    'learning_rate': lr
                }
                self.update_log.append(update_info)

                print(f"âœ… TEMPLATE UPDATED at frame {self.frame_count}, "
                      f"response: {max_response:.3f}, "
                      f"normalized: {normalized_response:.3f}, "
                      f"adaptive_threshold: {adaptive_threshold:.3f}")

            except Exception as e:
                print(f"âŒ Template update failed: {e}")
        else:
            if self.cfg.show_update_conditions:
                reasons = []
                if not condition_interval:
                    reasons.append("interval")
                if not condition_confidence:
                    reasons.append(f"confidence (need >{self.cfg.quality_threshold})")
                if not condition_adaptive:
                    reasons.append(f"adaptive (need >{adaptive_threshold:.3f})")

                if reasons:
                    print(f"â© Skip update at frame {self.frame_count}: " + ", ".join(reasons))

    def _handle_occlusions(self, max_response):
        """GÃ©rer les occlusions et Ã©viter les mises Ã  jour erronÃ©es"""
        # Calculer un seuil d'occlusion basÃ© sur l'historique
        if len(self.response_history) > 5:
            avg_response = np.mean(self.response_history)
            occlusion_threshold = avg_response * 0.3  # 30% de la rÃ©ponse moyenne
        else:
            occlusion_threshold = 2.0  # Seuil par dÃ©faut

        occlusion_detected = max_response < occlusion_threshold

        if occlusion_detected and self.cfg.show_update_conditions:
            print(f"ğŸš« OCCLUSION DETECTED at frame {self.frame_count}, "
                  f"response: {max_response:.3f}, "
                  f"threshold: {occlusion_threshold:.3f}")

        return occlusion_detected

    def get_update_statistics(self):
        """Obtenir les statistiques des mises Ã  jour"""
        if not self.update_log:
            return "No template updates performed"

        responses = [log['response'] for log in self.update_log]
        frames = [log['frame'] for log in self.update_log]

        stats = (f"Template Update Statistics:\n"
                 f"  Total updates: {len(self.update_log)}\n"
                 f"  Update frames: {frames}\n"
                 f"  Avg response: {np.mean(responses):.3f}\n"
                 f"  Min response: {np.min(responses):.3f}\n"
                 f"  Max response: {np.max(responses):.3f}")

        return stats

    @torch.no_grad()  # ä¸‹é¢çš„å®šä¹‰ç±»ï¼Œä¸ç”¨è®¡ç®—æ¢¯åº¦ï¼Œä¹Ÿä¸åšåå‘ä¼ æ’­ï¼ˆspeed upï¼‰
    def init(self, img, box):  # trackerä¸­éœ€è¦é‡æ–°å†™çš„ä¸¤ä¸ªæ–¹æ³•ä¹‹ä¸€
        # set to evaluation mode è®¾ä¸ºè¯„ä¼°æ¨¡å¼
        self.net.eval()  # æŠŠBNå’ŒDropoutå›ºå®šï¼Œä¸å†å–å¹³å‡æˆ–è°ƒæ•´ï¼Œç›´æ¥ç”¨è®­ç»ƒå¥½çš„å‚æ•°å€¼

        # convert box to 0-indexed and center based [y, x, h, w]
        box = np.array([  # from[ltx,lty,w,h] -> [cy,cx,h,w]
            box[1] - 1 + (box[3] - 1) / 2,  # ok but why minus 1??
            box[0] - 1 + (box[2] - 1) / 2,
            box[3], box[2]], dtype=np.float32)
        self.center, self.target_sz = box[:2], box[2:]  # è®°å½•bboxçš„ä¸­å¿ƒå’Œå®½é«˜sizeä¿¡æ¯ï¼Œä»¥å¤‡åç”¨

        # create hanning window  æ±‰å®çª—ä¹Ÿå«ä½™å¼¦çª—ï¼Œè®ºæ–‡ä¸­è¯´æ˜¯å¢åŠ æƒ©ç½š
        self.upscale_sz = self.cfg.response_up * self.cfg.response_sz  # ä¸Šé‡‡æ ·åˆ°16*17=272
        self.hann_window = np.outer(  # å¤–ç§¯ï¼Œnp.outer([m],[n])åˆ™ç”Ÿäº§mè¡Œnåˆ—çš„æ•°ç»„,è¡Œå…ƒç´ ä¸ºm[i]*nçš„æ¯ä¸€ä¸ªå…ƒç´ 
            np.hanning(self.upscale_sz),  # é«˜ç»´æ•°ç»„ä¼šè‡ªåŠ¨flattenæˆ1ç»´ç„¶åè¿›è¡Œè®¡ç®— æœ€åçš„å½¢å¼æ˜¯åˆ†å—æ•°ç»„ç»„æˆçš„æ•°ç»„
            np.hanning(self.upscale_sz))
        self.hann_window /= self.hann_window.sum()  # å½’ä¸€åŒ–

        # search scale factors ç”Ÿæˆå°ºåº¦æ± ï¼Œå°ºåº¦å› å­åœ¨ä¸€å®šèŒƒå›´å†…å‡åŒ€é€’å¢ï¼ˆä¸å¤ªæ‡‚ï¼Œæœ‰å•¥ç”¨ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
        self.scale_factors = self.cfg.scale_step ** np.linspace(  # np.linspace(start,stop,num)ç”Ÿæˆå‡åŒ€é—´éš”çš„numä¸ªæ•°å€¼åºåˆ—
            -(self.cfg.scale_num // 2),  # **ç­‰ä»·äºnp.pow() å¹‚è¿ç®—
            self.cfg.scale_num // 2,
            self.cfg.scale_num)  # np.linspace[-1 1 3] æ‰€ä»¥æ˜¯self.scale_factors = [1.0375^(-1), 1.0375^0, 1.0375^1] -> [
        # 0.9638 1 1.0375 ]

        # exemplar and search sizes  æ¨¡æ¿å’Œæœç´¢å›¾åƒå¤§å°
        context = self.cfg.context * np.sum(self.target_sz)  # contextå°±æ˜¯paddingå‡ºæ¥çš„åŒºåŸŸ 0.5(w+h)ï¼Œè¾¹ç•Œçš„è¯­ä¹‰ä¿¡æ¯
        # ä¸ºäº†è®¡ç®—z_szå’Œx_szï¼Œæœ€åé€å…¥crop_and_resizeå»æŠ å‡ºæœç´¢åŒºåŸŸï¼Œå…¶ä¸­æŠ å‡ºçš„z_sizeå¤§å°çš„ä½œä¸ºexemplar imageï¼Œå¹¶é€å…¥backbone
        # è¾“å‡ºembeddingï¼Œä¹Ÿå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªå›ºå®šçš„äº’ç›¸å…³kernelï¼Œä¸ºäº†ä¹‹åçš„ç›¸ä¼¼åº¦è®¡ç®—ç”¨ï¼Œ
        self.z_sz = np.sqrt(np.prod(self.target_sz + context))  # sqrt[(w+0.5(w+h))*(h+0.5(w+h))]
        self.x_sz = self.z_sz * \
                    self.cfg.instance_sz / self.cfg.exemplar_sz  # è¾“å…¥xåº”è¯¥å›ºå®šä¸º255ï¼Œæ¨¡æ¿zå›ºå®šä¸º127ï¼Œæ‰€ä»¥è¾“å…¥çš„æ¨¡æ¿zä¹˜ä¸Šä¸€ä¸ªç¼©æ”¾æ¯”ä¾‹å˜255ï¼Œä½†æ¨¡æ¿z
        # å› ä¸ºpaddingäº†ï¼Œæ‰€ä»¥æœ€åå¾—åˆ°çš„xå°ºåº¦æ˜¯255é™„è¿‘

        # exemplar image    z=æ¨¡æ¿å›¾åƒ
        self.avg_color = np.mean(img, axis=(0, 1))  # ç®—åƒç´ å¹³å‡ï¼Œç”¨æ¥paddingçš„
        z = ops.crop_and_resize(  # resizeåˆ°å›ºå®šçš„127æˆ–255
            img, self.center, self.z_sz,
            out_size=self.cfg.exemplar_sz,
            border_value=self.avg_color)

        # exemplar features   æ¨¡æ¿ç‰¹å¾ [w,h,c] -> [c,w,h]  ï¼ˆ127,127,3ï¼‰
        z = torch.from_numpy(z).to(
            # æ¢æˆgpuèƒ½å¤„ç†çš„tensor ä½†æ˜¯ä¸ºä»€ä¹ˆè¦åšpermuteï¼Ÿ è§£å†³ï¼š [w,h,c] -> [c,w,h]  !!!å› ä¸ºgot10kçš„æ•°æ®ç±»å‹æ˜¯ncwhï¼Œæ‰€ä»¥ç”¨è¿™ä¸ª
            self.device).permute(2, 0, 1).unsqueeze(0).float()  # unsqueeze() ï¼Œåˆ™ç»´åº¦å˜[mini-batch,c,w,h] NCWHæ ¼å¼ï¼Œæ•°æ®ç±»å‹float
        z1 = self.net.backbone(z)  # cropçš„zç»è¿‡backboneï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰è¾“å‡ºzçš„featureï¼Œè¿™é‡Œbackboneç”¨çš„æ˜¯VGG166
        z2 = self.net.backbone1(z)  # cropçš„zç»è¿‡backboneï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰è¾“å‡ºzçš„featureï¼Œè¿™é‡Œbackboneç”¨çš„æ˜¯AlexNet_V1
        # print("for breakpoint")
        # self.kernerl = z
        # z1 = self.net.dcn(z1)
        # z2 = self.net.dcn(z2)

        zf1 = self.net.tematt(z1)
        zf11 = self.net.attse(z1)
        zf11 = zf11 + z1
        self.kernel1 = zf1 + zf11

        zf2 = self.net.tematt(z2)
        zf22 = self.net.attse(z2)
        zf22 = zf22 + z2
        self.kernel2 = zf2 + zf22

        # Stocker le template initial
        self.current_template_z1 = self.kernel1.clone()
        self.current_template_z2 = self.kernel2.clone()
        self.best_response = 1.0  # RÃ©ponse maximale au dÃ©but
        self.frame_count = 0
        self.response_history = [max_response] if hasattr(self, 'max_response') else [5.0]  # Valeur initiale
        self.update_log = []  # RÃ©initialiser l'historique

        if self.cfg.show_update_conditions:
            print(f"ğŸ¯ INITIALIZATION at frame 0 - Template ready for tracking")

    #  ***self.kernelå°±ç”¨ç¬¬ä¸€å¸§åˆå§‹åŒ–äº†ï¼Œåé¢ä¸ä¼šå˜ï¼Œç”¨ä¸å’Œå½“å‰å¸§åšconv2då·ç§¯å¾—åˆ°å“åº”ã€‚åˆ°è¿™é‡Œåˆå§‹åŒ–å°±å®Œæˆäº†ï¼Œä¸»è¦æ˜¯è®¾å®šæ¨¡æ¿zã€‚å³Siameseçš„ä¸ŠåŠéƒ¨åˆ†

    # updataå¯¹åç»­å¸§æ›´æ–°å‡ºbboxæ¥ï¼Œå› ä¸ºæ˜¯tracking phaseï¼ˆè·Ÿè¸ªé˜¶æ®µï¼‰ï¼Œæ‰€ä»¥æŠŠæ¨¡å‹è®¾æˆeval modeã€‚ç„¶ååœ¨è¿™æ–°çš„å¸§é‡ŒæŠ å‡ºsearch imagesï¼Œ
    # æ ¹æ®ä¹‹å‰inité‡Œç”Ÿæˆçš„3ä¸ªå°ºåº¦ï¼Œç„¶åresizeæˆ255*255ï¼Œsearch imagesåœ¨resizeä¹‹å‰çš„è¾¹é•¿x_szå¤§çº¦ä¸ºtarget_szçš„4å€
    @torch.no_grad()  # è¿™é‡Œæ˜¯åšè·Ÿè¸ªçš„æ—¶å€™æ–°çš„å¸§è¿›æ¥åˆ™åšå¤„ç†ï¼Œç›´æ¥è·Ÿè¸ªï¼Œæ‰€ä»¥ä¸å†åšæ¢¯åº¦ï¼Œå¹¶ä¸”å†»ç»“ç½‘ç»œä¸­çš„batchnumå’Œdropoutç­‰å‚æ•°
    def update(self, img):
        # set to evaluation mode è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.net.eval()

        # search images  æœç´¢å›¾ç‰‡ å°†3ä¸ªå°ºåº¦çš„patchï¼ˆä¹Ÿå°±æ˜¯3ä¸ªæœç´¢èŒƒå›´ï¼‰æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œé€å…¥backboneï¼Œç”Ÿæˆemddingå
        # ä¸ä¹‹å‰çš„kernelè¿›è¡Œäº’ç›¸å…³ï¼Œå¾—åˆ°score mapï¼Œå¾—åˆ°3ä¸ª17*17çš„responsesï¼Œç„¶åå¯¹æ¯ä¸€ä¸ªresponseè¿›è¡Œä¸Šé‡‡æ ·åˆ°272*272
        x = [ops.crop_and_resize(
            img, self.center, self.x_sz * f,
            out_size=self.cfg.instance_sz,
            border_value=self.avg_color) for f in self.scale_factors]  # è¿™é‡Œè¾“å‡ºçš„xæ˜¯ä¸‰ç§å°ºåº¦çš„patchï¼Œæ˜¯ä¸‰ä¸ªæ•°ç»„
        # s_sc1 = torch.from_numpy(x[0]).to(self.device).permute(2, 0, 1).unsqueeze(0).float()
        # s_sc2 = torch.from_numpy(x[1]).to(self.device).permute(2, 0, 1).unsqueeze(0).float()
        # s_sc3 = torch.from_numpy(x[2]).to(self.device).permute(2, 0, 1).unsqueeze(0).float()
        x = np.stack(x, axis=0)  # æŠŠä¸‰ä¸ªæ•°ç»„å †å ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œæ­¤æ—¶shape=(3,255,255,3) ç¬¬ä¸€ä¸ª3æ˜¯ä¸‰å¼ patchå¯¹åº”çš„3ç»´ï¼Œç¬¬äºŒä¸ª3æ˜¯rgbçš„3ç»´ï¼Œ255 255æ˜¯æ¯å¼ xçš„h w
        x = torch.from_numpy(x).to(
            self.device).permute(0, 3, 1, 2).float()  # [patch_num, channels, h, w] å³patch_num CHW

        # responses è¿™é‡Œçš„responsesæ˜¯åŒ…å«3ç§å°ºåº¦å˜æ¢åçš„å“åº”ï¼Œåé¢é€šè¿‡æœ€å¤§å“åº”æ¥ç¡®å®šç”¨å“ªå¼ responseï¼Œå³å“ªä¸ªå°ºåº¦
        # a,b,c,d = s_sc1.size()
        x1 = self.net.backbone(x)  # å¾—åˆ°featureï¼Œæ­¤æ—¶x=[3*22*22*128]   -ã€‹ï¼ˆ3,256,22,22ï¼‰ï¼Ÿï¼Ÿ
        x2 = self.net.backbone1(x)  # å¾—åˆ°featureï¼Œæ­¤æ—¶x=[3*22*22*128]

        # x1 = self.net.dcn(x1)
        # x2 = self.net.dcn(x2)

        xf1 = self.net.detatt(x1)
        xf11 = self.net.attse(x1)
        xf11 = xf11 + x1
        x1 = xf1 + xf11

        xf2 = self.net.detatt(x2)
        xf22 = self.net.attse(x2)
        xf22 = xf22 + x2
        x2 = xf2 + xf22

        # self.kernel, x= self.net.feature_enhance(self.kernel,x)
        # x1 = self.net.feature_enhance(self.kernel, x[1])
        # x2 = self.net.feature_enhance(self.kernel, x[2])

        responses1 = self.net.head(self.kernel1,
                                   x1)  # æ­¤æ—¶x=[3*17*17*128] headå°±æ˜¯SiameseNetï¼Œè®¡ç®—å½“å‰å¸§ä¸æ¨¡æ¿zçš„ç›¸å…³å“åº”ï¼ˆå·ç§¯å“åº”ï¼‰ï¼Œæ¨¡æ¿zåˆ™æ˜¯å‰é¢initåˆå§‹åŒ–å¥½çš„self.kernel
        responses2 = self.net.head(self.kernel2,
                                   x2)  # æ­¤æ—¶x=[3*17*17*128] headå°±æ˜¯SiameseNetï¼Œè®¡ç®—å½“å‰å¸§ä¸æ¨¡æ¿zçš„ç›¸å…³å“åº”ï¼ˆå·ç§¯å“åº”ï¼‰ï¼Œæ¨¡æ¿zåˆ™æ˜¯å‰é¢initåˆå§‹åŒ–å¥½çš„self.kernel

        # responses1 = self.net.dcn(responses1)
        # responses2 = self.net.dcn(responses2)

        out1 = responses1
        out2 = responses2

        responses1 = out1 + out2
        responses2 = out1 * out2

        responses = self.net.conv(torch.cat([responses1, responses2],
                                            dim=1))  # æ­¤æ—¶x=[3*17*17*128] headå°±æ˜¯SiameseNetï¼Œè®¡ç®—å½“å‰å¸§ä¸æ¨¡æ¿zçš„ç›¸å…³å“åº”ï¼ˆå·ç§¯å“åº”ï¼‰ï¼Œæ¨¡æ¿zåˆ™æ˜¯å‰é¢initåˆå§‹åŒ–å¥½çš„self.kernel
        # responses = responses1 + responses2
        # responsesa = self.net.satt(responses)
        # responses = responses+responsesa
        # responses = 0.3 * self.net.head(self.kernel1, x1) + 0.7 * self.net.head(self.kernel2, x2)
        responses = responses.squeeze(1).cpu().numpy()  # åˆ æ‰1ç»´ï¼ˆchannelsï¼‰ æ­¤æ—¶responses.shape=[3,17,17]    ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
        # ******æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

        # upsample responses and penalize scale changes  å¢åŠ ç›¸åº”æ ·æœ¬å¹¶æƒ©ç½šè§„æ¨¡å˜åŒ–
        responses = np.stack([cv2.resize(  # æŠŠ3*17*17 resizeåˆ° 3*272*272 (åé¢è¦æ˜ å°„å›åˆ°åŸimage)
            u, (self.upscale_sz, self.upscale_sz),  # åšä¸Šé‡‡æ ·ï¼Œç”¨çš„æ˜¯ä¸‰æ¬¡æ ·æ¡æ’å€¼
            interpolation=cv2.INTER_CUBIC)
            for u in responses])
        # å¯¹å°ºåº¦è¿›è¡Œæƒ©ç½š
        responses[
            :self.cfg.scale_num // 2] *= self.cfg.scale_penalty  # responsesåŒ…å«ä¸‰å¼ å°ºåº¦çš„responseï¼Œå¯¹ç¬¬ä¸€å¼ å“åº”å’Œç¬¬ä¸‰å¼ å“åº”åšæƒ©ç½šï¼Œè€ŒåŸå°ºå¯¸å“åº”ä¸å˜  ä¸ºä»€ä¹ˆï¼Ÿ
        responses[self.cfg.scale_num // 2 + 1:] *= self.cfg.scale_penalty  #
        # peak scale  å³°å€¼å°ºåº¦
        scale_id = np.argmax(np.amax(responses, axis=(1, 2)))  # åˆ†åˆ«æ‰¾ä¸‰å¼ å›¾çš„æœ€å¤§å“åº”ï¼Œç„¶åå†ä»è¿™ä¸‰ä¸ªå“åº”ä¸­æ‰¾æœ€å¤§å“åº”ï¼Œå°ºåº¦å› å­åˆ™ç”¨è¿™å¼ æœ€å¤§å“åº”æ‰€å¯¹åº”çš„
        #                                                  (1,2)å¯¹äºå›¾çš„whï¼Œå³æ‰¾æ¯ä¸€å¼ å›¾çš„ï¼Œ0å¯¹åº”ç¬¬å‡ å¼ 

        # peak location  å³°å€¼ä½ç½®
        response = responses[scale_id]  # é€‰æœ€å¤§å“åº”å¯¹åº”çš„é‚£å¼ 
        max_response = response.max()  # Stocker la rÃ©ponse maximale pour le template update

        # GÃ©rer les occlusions et mettre Ã  jour le template
        if not self._handle_occlusions(max_response):
            self._update_template(img, max_response, scale_id)

        response -= response.min()  # éƒ½å‡å»æœ€å°å“åº”å€¼ ä¸€ç§å¹³æ»‘å¤„ç†å§
        response /= response.sum() + 1e-16  # å½’ä¸€åŒ– ä¸ºä»€ä¹ˆè¦åŠ 1e-16ï¼Ÿ
        response = (1 - self.cfg.window_influence) * response + \
                   self.cfg.window_influence * self.hann_window  # è¾¹ç•Œæ•ˆåº”çš„å¤„ç†
        # æ‰¾åˆ°ä¸€å¼ responseä¸Šå³°å€¼ç‚¹ï¼ˆpeak locationï¼‰
        loc = np.unravel_index(response.argmax(),
                               response.shape)  # ä¸­å¿ƒç‚¹ cropedçš„patch (y,x) ä»¥patchä¸ºæ ‡å°ºçš„centerï¼Œåé¢è¿˜è¦æ˜ å°„å›åŸimage

        # locate target center è¿™é‡Œçš„è®¡ç®—ä¸å¤ªçœ‹å¾—æ‡‚ï¼ˆæ‡‚å•¦ï¼ï¼ï¼‰ å°±æ˜¯é€†å›å»æŠŠcenteræ˜ å°„åˆ°åŸå›¾ç‰‡ å› ä¸ºå›¾ç‰‡å·²ç»ç»è¿‡äº†ä¸Šé‡‡æ · cropç­‰æ“ä½œ [138 134]->[2.5 -1.5]->[1.25 -0.75]->[0.45269844 -0.27161906]->[110.433945 61.620953]
        disp_in_response = np.array(loc) - (
                self.upscale_sz - 1) / 2  # å€’æ•°ç¬¬ä¸€æ­¥æ˜¯upsamplingï¼Œæ‰€ä»¥å…ˆæ˜ å°„å›æœªupsamplingæ—¶çš„centerï¼Œå³°ä¹‹ç‚¹å’Œresponseä¸­å¿ƒçš„ä½ç§»
        disp_in_instance = disp_in_response * \
                           self.cfg.total_stride / self.cfg.response_up  # æ˜ å°„å›backbondä¹‹å‰çš„center backbondå¤šå±‚å·ç§¯æ‰€æœ‰çš„strideä¸ºtotal_strideï¼Œæ‰€ä»¥è¦ä¹˜å›å»å˜å›ä¹‹å‰çš„å°ºå¯¸
        #                                                        å†æ˜ å°„å›æ²¡åšresizeä¹‹å‰çš„patch
        # æ ¹æ®disp_in_imageä¿®æ•´centerï¼Œç„¶åupdata target size
        disp_in_image = disp_in_instance * self.x_sz * \
                        self.scale_factors[
                            scale_id] / self.cfg.instance_sz  # ä»patchæ˜ å°„å›æ²¡åšpaddingä¹‹å‰çš„centerï¼Œå³åœ¨åŸimageçš„center
        # è¿™é‡Œçš„disp_in_xxéƒ½æ˜¯ä¸€ä¸ªç›¸å¯¹çš„å·®å€¼ï¼Œæœ€åç”¨åŸcenteråŠ ä¸Šè¿™ä¸ªå·®å€¼å°±å¾—åˆ°æ˜ å°„å›æ¥çš„centeräº†
        self.center += disp_in_image

        # update target size å°±æ˜¯ç¨€ç–æ›´æ–°é‚£ä¸ªå¼å­ ï¼ˆ1-å°ºåº¦å­¦ä¹ ç‡ï¼‰+å°ºåº¦å­¦ä¹ ç‡Ã—å°ºåº¦å› å­
        scale = (1 - self.cfg.scale_lr) * 1.0 + \
                self.cfg.scale_lr * self.scale_factors[scale_id]
        self.target_sz *= scale  # æŒ‰ç…§å°ºåº¦å› å­æ›´æ–°å°ºåº¦
        self.z_sz *= scale  # æŒ‰ç…§å°ºåº¦å› å­æ›´æ–°å°ºåº¦
        self.x_sz *= scale  # æŒ‰ç…§å°ºåº¦å› å­æ›´æ–°å°ºåº¦

        # return 1-indexed and left-top based bounding box
        box = np.array([  # boxè¿˜åŸå›ï¼šé¡¶è§’ç‚¹ w h å› ä¸ºè¦å’Œè®¡ç®—è·Ÿè¸ªæ€§èƒ½ï¼Œæ•°æ®é›†æä¾›çš„æ•°æ®æ˜¯[é¡¶è§’ç‚¹ w h]çš„æ ¼å¼
            self.center[1] + 1 - (self.target_sz[1] - 1) / 2,
            self.center[0] + 1 - (self.target_sz[0] - 1) / 2,
            self.target_sz[1], self.target_sz[0]])

        return box

    # è¯»frame->crop&resize(with scale)->features->do conv with z->response->choose best scale resopnse->
    # updata paras-> return boxes -> visualize
    def track(self, img_files, box, visualize=False):  # ä¼ å…¥è§†é¢‘åºåˆ—å’Œç¬¬ä¸€å¸§çš„bbox ç„¶åé€šè¿‡æ¨¡å‹ï¼Œå¾—åˆ°åç»­å¸§çš„ç›®æ ‡ä½ç½®ï¼Œ
        # ä¸»è¦ç”±initå’Œupdataå‡½æ•°å®ç°ï¼Œè¿™ä¹Ÿæ˜¯å‡»æ²‰trackeréœ€è¦é‡å†™çš„ä¸¤ä¸ªæ–¹æ³•
        frame_num = len(img_files)
        boxes = np.zeros((frame_num, 4))
        boxes[0] = box
        times = np.zeros(frame_num)

        for f, img_file in enumerate(img_files):
            img = ops.read_image(img_file)

            begin = time.time()  # å¼€å§‹æ—¶é—´
            if f == 0:
                self.init(img, box)  # ç¬¬ä¸€å¸§æ—¶åšåˆå§‹åŒ–ï¼Œç”Ÿæˆæ¨¡æ¿zï¼ˆself.kernel)
            else:
                boxes[f, :] = self.update(img)  # è®¡ç®—æ¯ä¸€å¸§imgè¿”å›çš„boxï¼Œbox=[tlx,tly,w,h] å­˜åˆ°boxesé‡Œï¼Œç”¨æ¥åšå¯è§†åŒ–
                # self.init(img, boxes[f])
            times[f] = time.time() - begin

            if visualize:
                ops.show_image(img, boxes[f, :])
            # times[f] = f/(time.time() - begin)

        # Afficher les statistiques finales
        if self.cfg.show_update_conditions:
            print("\n" + "=" * 50)
            print(self.get_update_statistics())
            print("=" * 50)

        return boxes, times

# å…·ä½“åœ¨è¿™ä¸ªå‡½æ•°é‡Œé¢å®ç°äº†è®­ç»ƒå’Œåå‘ä¼ æ’­
    def train_step(self, batch, backward=True):
        # set network mode
        self.net.train(backward)

        # parse batch data
        z = batch[0].to(self.device, non_blocking=self.cuda)  # z.shape: torch.Size([8,3,127,127])   *NCWHæ ¼å¼
        x = batch[1].to(self.device, non_blocking=self.cuda)  # x.shape: torch.Size([8,3,239,239])

        with torch.set_grad_enabled(backward):
            # inference
            responses = self.net(z, x)  # responses.shape: torch.Size([8, 1, 15, 15])

            # calculate loss
            labels = self._create_labels(responses.size())  # è¦æŠŠlabels resizeæˆresponsesçš„sizeï¼Œå› ä¸ºåé¢è®¡ç®—lossçš„æ—¶å€™æ˜¯element-wiseçš„
            loss = self.criterion(responses, labels)

            if backward:
                # back propagation
                self.optimizer.zero_grad()
                if hasattr(torch.cuda, 'empty_cache'):
                    torch.cuda.empty_cache()

                loss.backward()
                if hasattr(torch.cuda, 'empty_cache'):
                    torch.cuda.empty_cache()
                self.optimizer.step()

        return loss.item()



    @torch.enable_grad()
    def train_over(self, seqs, val_seqs=None,
                   save_dir='models'):   #save_diræ¨¡å‹ä¿å­˜çš„ä½ç½®
        # set to train mode è®¾ç½®ä¸ºtrainæ¨¡å¼ï¼Œå‚æ•°å¯è¿­ä»£
        self.net.train()

        # create save_dir folder
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # setup dataset
        transforms = SiamFCTransforms(
            exemplar_sz=self.cfg.exemplar_sz,
            instance_sz=self.cfg.instance_sz,
            context=self.cfg.context)
        # è¿™é‡Œçš„datasetç»è¿‡Pairæ˜¯è¿”å›ä¸€å¯¹ï¼ˆz,xï¼‰ï¼Œä¸”å·²ç»ç»è¿‡è£å‰ªæˆä¸ºåˆæ ¼çš„è¾“å…¥
        dataset = Pair(
            seqs=seqs,
            transforms=transforms)

        # setup dataloader
        dataloader = DataLoader(
            dataset,
            batch_size=self.cfg.batch_size,  # batch_size = 8
            shuffle=True,  # batchå–åˆ°çš„å¯ä»¥æ˜¯ä¸å…¶å®ƒbatché‡å¤çš„ï¼Œè¿™æ ·ä¼šå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œæœ‰çš„dataå¯èƒ½ä»å¤´åˆ°å°¾éƒ½æ²¡æœ‰è¢«æŠ½åˆ°
            num_workers=self.cfg.num_workers,  # åŠ è½½æ•°æ®çš„æ¬¡æ•°ï¼ŒæŒ‰cfgè®¾ä¸º32ï¼Œåˆ™åˆ†32æ¬¡åŠ è½½ï¼ˆæ˜¯ä¸æ˜¯å°±æ˜¯batchï¼Ÿï¼Ÿï¼‰
            pin_memory=self.cuda,  # copy Tensors into CUDA
            drop_last=True)  # æœ€åä¸€ä¸ªbatch dropæ‰

        # loop over epochså¸¸è§„æ“ä½œ   train_stepå®ç°äº†è®­ç»ƒå’Œåå‘ä¼ æ’­ å…³é”®åœ°æ–¹ï¼Œå¦‚ä¸Šã€‚æ•°æ®å‡†å¤‡å¥½äº†ï¼Œç»è¿‡å˜æ¢åŠ è½½è¿›æ¥å°±å¯ä»¥è®­ç»ƒäº†
        for epoch in range(self.cfg.epoch_num):
            # update lr at each epoch
            #self.lr_scheduler.step(epoch=epoch)

            # loop over dataloader
            for it, batch in enumerate(
                    dataloader):  # enumerateæšä¸¾dataloaderä¸­çš„å…ƒç´ ï¼Œè¿”å›å€¼åŒ…æ‹¬indexå’Œdatas åˆ™itå¯¹åº”indexï¼Œbatchå¯¹åº”dataloader
                # Epoch: 49[it+1: 914 / len(dataloader): 1166] Loss: 0.19454
                loss = self.train_step(batch, backward=True)
                print('Epoch: {} [{}/{}] Loss: {:.5f}'.format(
                    epoch + 1, it + 1, len(dataloader), loss))
                sys.stdout.flush()
            # save checkpoint
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)
            net_path = os.path.join(
                save_dir, 'siamfc_alexnet_e%d.pth' % (epoch + 1))
            torch.save(self.net.state_dict(), net_path)

            self.lr_scheduler.step(epoch=epoch)




    #åˆ›å»ºæ ‡ç­¾ï¼Œå› ä¸ºzï¼Œxéƒ½æ˜¯ä»¥ç›®æ ‡ä¸ºä¸­å¿ƒçš„ï¼Œæ‰€ä»¥labelsçš„ä¸­å¿ƒä¸º1ï¼Œä¸­å¿ƒä»¥å¤–ä¸º0   ï¼ˆè¿˜ä¸å¤ªæ‡‚ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
    def _create_labels(self, size):  # train stepé‡Œç”¨äºç”Ÿæˆå½“å‰responseçš„labelç„¶åä¸gtçš„labelç®—lossï¼Œç„¶åbackward
        # skip if same sized labels already created
        if hasattr(self,
                   'labels') and self.labels.size() == size:  # hasattræ‰¾å±æ€§ï¼Œå¦‚æœselfé‡Œæœ‰labelsè¿™ä¸ªå±æ€§åˆ™è¿”å›çœŸ; selfé‡Œçš„labelså±æ€§sizeå’Œç»™å®šçš„ç›¸åŒï¼Œåˆ™ç›´æ¥è¿”å›labels
            return self.labels  # è¿™é‡Œçš„sizeä¸»è¦æ˜¯ç»´åº¦æ˜¯å¦ä¸€è‡´ï¼Œåé¢çš„ç›®çš„å°±æ˜¯æ‰©å±•ç»´åº¦ä½¿ä¸€è‡´

        def logistic_labels(x, y, r_pos, r_neg):
            dist = np.abs(x) + np.abs(y)  # block distance
            labels = np.where(dist <= r_pos,  # np.where åˆ¤æ–­0/1,ç›¸åº”è¿”å›0/1
                              np.ones_like(x),  # æ»¡è¶³dist<=r_negåˆ™è¿”å›np.ones_like(x) åä¹‹è¿”å›åé¢çš„è¾“å‡º
                              np.where(dist < r_neg,
                                       np.ones_like(x) * 0.5,
                                       np.zeros_like(x)))
            return labels  # åœ¨è®¾å®šçš„positiveèŒƒå›´å†…åƒç´ ç‚¹å€¼è®¾ä¸º1, åœ¨positiveå¤–åˆæœªåˆ°negativeåˆ™è®¾ä¸º0.5,å…¶ä½™è®¾ä¸º0

        # distances along x- and y-axis
        n, c, h, w = size
        x = np.arange(w) - (w - 1) / 2  # ç”Ÿæˆä¸€ä¸ª0-wçš„å›ºå®šæ­¥é•¿æ’åˆ—ï¼Œstride=1 ->> ç®—ä¸­å¿ƒç‚¹ï¼ˆä¸‹åŒï¼‰
        y = np.arange(h) - (h - 1) / 2
        x, y = np.meshgrid(x, y)  # ç”Ÿæˆç½‘æ ¼ç‚¹åæ ‡çŸ©é˜µ but why????

        # create logistic labels è®ºæ–‡2.2å°èŠ‚
        r_pos = self.cfg.r_pos / self.cfg.total_stride  # radius of pos æ¢ç®—åˆ°original imageä¸Šçš„posåŠå¾„
        r_neg = self.cfg.r_neg / self.cfg.total_stride  # r_neg 0
        labels = logistic_labels(x, y, r_pos, r_neg)  # è¿”å›ä¸€å¼ å›¾ï¼Œä»…æœ‰ä¸­å¿ƒéƒ¨åˆ†ä½ç½®æ˜¯0

        # repeat to size
        labels = labels.reshape((1, 1, h, w))
        labels = np.tile(labels, (n, c, 1, 1))  # labelsçš„å››ä¸ªç»´åº¦åˆ†åˆ«æŒ‰sizeæ‰©å±•ï¼Œå°±å’Œå‰é¢åˆ¤æ–­labelsçš„sizeæ˜¯å¦ç›¸åŒå¯¹å¾—ä¸Šäº†
        # nä¸ªbatchï¼Œæ¯ä¸ªbatch cä¸ªé€šé“ æ¯ä¸ªé€šé“æ˜¯1Ã—1æ•°ç»„ï¼ˆå°±æ˜¯ä¸€ä¸ªç‚¹å˜›ï¼‰
        # convert to tensors
        self.labels = torch.from_numpy(labels).to(self.device).float()  # numyp to tensor

        return self.labels